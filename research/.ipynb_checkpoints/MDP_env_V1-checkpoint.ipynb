{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80di-0BctsAg"
   },
   "source": [
    "## Génération du Markov Decision Process\n",
    "\n",
    "### Définition du problème :\n",
    "\n",
    "Un agent se trouve sur une case aléatoire d'une grille 4 $\\times$ 4, sur la majorité des case de cette grille se trouvent des malus de score qui sont appliqués lorsqu'on marche dessus. Sur une faible fraction des cases se trouve un trésor, qui offre une récompense. Le but de l'agent est de maximiser sa récompense, en outre, il doit trouver le plus court chemin vers le trésor. Voici la grille des états :\n",
    "\n",
    "$$grid = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 0 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12 \\\\\n",
    "0 & 14 & 15 & 16 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "On remarque que certains états sont impossibles, il y a des obstacles dessus, ce qui est traduit par une étoile sur la case correspondante.\n",
    "\n",
    "D'après la définition du problème, la fonction de récompense ne dépend que de la case sur laquelle l'agent se déplace à un certain instant. On a donc $R:S\\to \\mathbb{R}$. Voici la matrice de récompense :\n",
    "$$R=\\begin{bmatrix}\n",
    "-0.1 & -0.1 & -0.1 & -0.1 \\\\\n",
    "- 1 & 0 & -0.1 & -0.1 \\\\\n",
    "-0.1 & -0.1 & -0.1 & -0.1 \\\\\n",
    "0 & 1 & -0.1 & -0.1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Les actions que peut prendre l'agent sont des déplacements dans les 4 directions, cependant, ces actions ne sont pas déterministes, l'agent ne peut que \"essayer\" d'aller dans une direction. Toutefois, s'il essaie d'aller dans une direction, il n'est pas possible qu'il aille à l'opposé. Par exemple, si l'agent prend l'action \"essayer d'aller en haut\", il n'est pas possible qu'il aille en bas, par contre il peut aller à gauche ou à droite avec une probabilité faible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Zbr6k5qSFmT"
   },
   "source": [
    "### Import des librairies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6LZanjzt6uf"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23WLebUySLx8"
   },
   "source": [
    "### Définition des actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "De43ReqY7IdV"
   },
   "outputs": [],
   "source": [
    "## dans le sens des aiguilles d'une montre\n",
    "\n",
    "actions = {'up' : [0.8,0.1,0.,0.1], \n",
    "           'right':[0.1,0.8,0.1,0], \n",
    "           'down':[0.,0.1,0.8,0.1], \n",
    "           'left':[0.1,0.,0.1,0.8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FT3RVJMfSQR6"
   },
   "source": [
    "### Définition de la matrice représentative de la grille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grcI0sE2ROV7"
   },
   "outputs": [],
   "source": [
    "grid = np.array([[1,2,3,4],\n",
    "                 [5,0,7,8],\n",
    "                 [9,10,11,12],\n",
    "                 [0,14,15,16]]\n",
    "               )\n",
    "n_col = grid.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qq3D_Xl-SUqy"
   },
   "source": [
    "### Définition de la matrice d'adjacence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "FiQ5X0CiRPjS",
    "outputId": "03f3e87f-9921-46f2-de95-6f4fdc13e6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## cases numérotées de gauche à droite et de haut en bas\n",
    "\n",
    "n_states = np.prod(grid.shape)\n",
    "adj = np.zeros((n_states, n_states))\n",
    "\n",
    "for i in range (0, n_states) :\n",
    "    for j in range (0, n_states) :\n",
    "    # Si on est sur un état adjacent, et que cet état n'est pas obstrué, on met un 1 qui signifie qu'on peut aller sur cet état à partir\n",
    "    # de l'état où l'on est\n",
    "        if (j == i - n_col or j == i+1 or j == i + n_col or j == i-1) and grid[j//n_col,j%n_col]!=0 :\n",
    "            adj[i,j] = 1\n",
    "\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PVru__9SXZa"
   },
   "source": [
    "Une fois que l'on a la matrice d'adjacence, les matrices de transitions ne sont que des déclinaisons de cette matrice.\n",
    "### Définition des matrices de transition\n",
    "\n",
    "On considère que quelque soit l'état, on peut essayer d'aller dans toutes les directions. Cependant, les matrices de transitions doivent être stochastiques donc on modifie leur contenu de sorte que la somme de chaque ligne vale 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yq-7Zn7LRha2",
    "outputId": "4533c462-9f06-43ba-aa7a-07859d3c432c"
   },
   "outputs": [],
   "source": [
    "T = []\n",
    "adj_coor = [-n_col, 1,n_col, -1]\n",
    "for action in actions :\n",
    "    Ta = np.zeros((n_states,n_states))\n",
    "    p = actions[action]\n",
    "    for i in range (0, n_states) :\n",
    "        for k in range(len(p)):\n",
    "            j = i+adj_coor[k]\n",
    "            if 0<=j<=15:\n",
    "                Ta[i, j] = adj[i, j]*p[k]\n",
    "                Lsum = np.sum(Ta[i,:])\n",
    "        if Lsum !=1:\n",
    "            Ta[i,:]*=1/Lsum\n",
    "    T.append(Ta)\n",
    "\n",
    "T = np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.11111111 0.\n",
      " 0.         0.         0.         0.88888889]\n"
     ]
    }
   ],
   "source": [
    "print(T[2,11,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQny11x1SuzG"
   },
   "source": [
    "### Définition des matrices de récompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "8Oy_GgTsRiDk",
    "outputId": "24fdd32f-f931-4acb-cec0-6d385ded3675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1 -0.1 -0.1 -0.1 -1.   0.  -0.1 -0.1 -0.1 -0.1 -0.1 -0.1  0.   1.\n",
      " -0.1 -0.1]\n"
     ]
    }
   ],
   "source": [
    "R = np.array([-0.1,-0.1,-0.1,-0.1,-1,0,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0,1,-0.1,-0.1]).transpose()\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme de décision : Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre système par un Markov Decision Process (matrice de transition T, matrice de récompense R, états S, ici matrice adjacence et états).\n",
    "\n",
    "Nous cherchons la politique optimale avec l'algorithme de Policy Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de MDP_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
